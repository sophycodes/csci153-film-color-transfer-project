{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8700862-be5f-4cee-aae7-3a3e3d09dcbf",
   "metadata": {},
   "source": [
    "# Depth Anything Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b7a04c-b97e-418c-9104-ab69d10b8f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@343.291] global loadsave.cpp:268 findDecoder imread_('imgs/TATBILBframe_0095.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs/TATBILBframe_0095.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m raw_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n\u001b[0;32m---> 40\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_img\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# HxW raw depth map in numpy       \u001b[39;00m\n",
      "File \u001b[0;32m/mnt/cs/cs153/customenvs/stvenv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cs/cs153/projects/sophy_theo/./Depth-Anything-V2/depth_anything_v2/dpt.py:188\u001b[0m, in \u001b[0;36mDepthAnythingV2.infer_image\u001b[0;34m(self, raw_image, input_size)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfer_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_image, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m518\u001b[39m):\n\u001b[0;32m--> 188\u001b[0m     image, (h, w) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage2tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(image)\n\u001b[1;32m    192\u001b[0m     depth \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(depth[:, \u001b[38;5;28;01mNone\u001b[39;00m], (h, w), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/cs/cs153/projects/sophy_theo/./Depth-Anything-V2/depth_anything_v2/dpt.py:211\u001b[0m, in \u001b[0;36mDepthAnythingV2.image2tensor\u001b[0;34m(self, raw_image, input_size)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimage2tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_image, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m518\u001b[39m):        \n\u001b[1;32m    197\u001b[0m     transform \u001b[38;5;241m=\u001b[39m Compose([\n\u001b[1;32m    198\u001b[0m         Resize(\n\u001b[1;32m    199\u001b[0m             width\u001b[38;5;241m=\u001b[39minput_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         PrepareForNet(),\n\u001b[1;32m    209\u001b[0m     ])\n\u001b[0;32m--> 211\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m \u001b[43mraw_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    213\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(raw_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m    215\u001b[0m     image \u001b[38;5;241m=\u001b[39m transform({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.append('./Depth-Anything-V2')\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "# from tqdm import tqdm\n",
    "\n",
    "os.chdir('./Depth-Anything-V2')\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from depth_anything_v2.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "# automatically select the best available device for running PyTorch operations\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "# small 24.8M params, base 97.5M params, large 335.3M params, giant(not yet out)\n",
    "# encoders = ['vits', 'vitb', 'vitl', 'vitg']\n",
    "encoder = 'vits' # or 'vitl', 'vitb', 'vitg'\n",
    "\n",
    "#create an instance of depth anything model \n",
    "model = DepthAnythingV2(**model_configs[encoder])\n",
    "# load pretrained weights \n",
    "model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_{encoder}.pth', map_location='cpu'))\n",
    "# send model to device \n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "img_path = \"imgs/TATBILBframe_0095.jpg\"\n",
    "raw_img = cv2.imread(img_path)\n",
    "depth = model.infer_image(raw_img) # HxW raw depth map in numpy       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192caaa5-0a7b-4a7f-b696-f648ccb9e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cs/cs153/projects/sophy_theo/Depth-Anything-V2\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13341591-4a0b-41e0-956b-66be7f25ad31",
   "metadata": {},
   "source": [
    "# Segment Anything Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f1cea5-59e5-47b7-97ca-b846d0dc42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
